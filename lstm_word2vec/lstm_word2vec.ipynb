{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import associated libraries for data cleaning\n",
    "# Based on original text classification analysis on stories by: rantsandruse/lstm_word2vec\n",
    "from lib import data_split, features_word2vec\n",
    "from lib import read_article\n",
    "\n",
    "# Tokenization and stopword removal\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# For Neural Networks\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.callbacks import TensorBoard\n",
    "from losswise.libs import LosswiseKerasCallback\n",
    "\n",
    "# Checking Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Graphing and visualizing\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib import cm\n",
    "from pylab import savefig\n",
    "\n",
    "\n",
    "# Setting graphing preferences\n",
    "sns.set(style=\"darkgrid\", color_codes=True)\n",
    "\n",
    "# Printing\n",
    "import locale\n",
    "\n",
    "# Show plots locally\n",
    "locale.setlocale( locale.LC_ALL, '' )\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Read in and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34930, 5)\n",
      "(11644, 5)\n"
     ]
    }
   ],
   "source": [
    "songs_sample_train = pd.read_csv(r\"C:\\Users\\nites\\OneDrive\\UVA\\FallSemester\\SYS6018\\Lyric Analysis\\lyric_analysis\\lstm_word2vec\\data\\train.csv\")\n",
    "print(songs_sample_train.shape)\n",
    "songs_sample_test = pd.read_csv(r\"C:\\Users\\nites\\OneDrive\\UVA\\FallSemester\\SYS6018\\Lyric Analysis\\lyric_analysis\\lstm_word2vec\\data\\test.csv\")\n",
    "print(songs_sample_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46574, 5)\n"
     ]
    }
   ],
   "source": [
    "songs_sample = pd.concat([songs_sample_train, songs_sample_test])\n",
    "print(songs_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>full_lyrics</th>\n",
       "      <th>chart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17599</td>\n",
       "      <td>poinciana</td>\n",
       "      <td>steve lawrence</td>\n",
       "      <td>blow tropic wind sing a song to the trees tree...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2383</td>\n",
       "      <td>live like you were dying</td>\n",
       "      <td>tim mcgraw</td>\n",
       "      <td>he said i was in my early forties with a lot o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40051</td>\n",
       "      <td>living legends</td>\n",
       "      <td>da band</td>\n",
       "      <td>dylan yo blaze the fire and watch the enemies ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18917</td>\n",
       "      <td>in the night side of eden</td>\n",
       "      <td>h i m</td>\n",
       "      <td>divided we stand in the light of a frozen sun ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19625</td>\n",
       "      <td>how insensitive</td>\n",
       "      <td>rosemary clooney</td>\n",
       "      <td>how insensitive i must have seemed when she to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                      title            artist  \\\n",
       "0       17599                  poinciana    steve lawrence   \n",
       "1        2383   live like you were dying        tim mcgraw   \n",
       "2       40051             living legends           da band   \n",
       "3       18917  in the night side of eden             h i m   \n",
       "4       19625            how insensitive  rosemary clooney   \n",
       "\n",
       "                                         full_lyrics  chart  \n",
       "0  blow tropic wind sing a song to the trees tree...      0  \n",
       "1  he said i was in my early forties with a lot o...      1  \n",
       "2  dylan yo blaze the fire and watch the enemies ...      0  \n",
       "3  divided we stand in the light of a frozen sun ...      0  \n",
       "4  how insensitive i must have seemed when she to...      0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "songs_sample = songs_sample.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate word2vec Embeddings and Training Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Defined path of word2vec model \n",
    "model_path = \"songs2vec\"\n",
    "model = features_word2vec.get_word2vec_model(songs_sample, \"full_lyrics\", num_features=300, downsampling=1e-3, model_name=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=18070, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create word embeddings\n",
    "vector_dim = 300\n",
    "embedding_weights = np.zeros((len(model.wv.vocab), vector_dim))\n",
    "for i in range(len(model.wv.vocab)):\n",
    "    embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_weights[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper Functions from the github repo\n",
    "def makeIndexVec(words, model, maxLength=50, withZeros=False, padLeft=True):\n",
    "    indexVec = np.zeros((maxLength,), dtype=\"float32\")\n",
    "\n",
    "    vocab = model.wv.vocab\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    if padLeft:\n",
    "        i = maxLength - 1\n",
    "        for word in reversed(words):\n",
    "            if i == 0:\n",
    "                break\n",
    "            if word in index2word_set:\n",
    "                indexVec[i] = vocab[word].index\n",
    "            i = i - 1\n",
    "    else:\n",
    "        i = 0\n",
    "        for word in words:\n",
    "            if word in index2word_set:\n",
    "                indexVec[i] = vocab[word].index\n",
    "            i = i + 1\n",
    "\n",
    "    return indexVec\n",
    "\n",
    "def get_indices_word2vec(data, column, model, maxLength=50, writeIndexFileName=\"./model/word2vec_indices.pickle\",\n",
    "                         padLeft=True, keep_freqwords=[]):\n",
    "    \n",
    "    songs = read_article.data_to_reviews(data, column, keep_freqwords=keep_freqwords)\n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    #\n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    songIndexVecs = np.zeros((len(songs), maxLength), dtype=\"int32\")\n",
    "    #\n",
    "    # Loop through the songs\n",
    "    for song in songs:\n",
    "        # Print a status message every 1000th song\n",
    "        if counter % 1000 == 0:\n",
    "            print(\"song %d of %d\" % (counter, len(songs)))\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        songIndexVecs[counter] = makeIndexVec(song, model, maxLength, padLeft=padLeft)\n",
    "        # Increment the counter\n",
    "        counter = counter + 1\n",
    "\n",
    "    # pickle.dump(songIndexVecs, open(writeIndexFileName, 'wb'))\n",
    "    return songIndexVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 34930\n",
      "Review 1000 of 34930\n",
      "Review 2000 of 34930\n",
      "Review 3000 of 34930\n",
      "Review 4000 of 34930\n",
      "Review 5000 of 34930\n",
      "Review 6000 of 34930\n",
      "Review 7000 of 34930\n",
      "Review 8000 of 34930\n",
      "Review 9000 of 34930\n",
      "Review 10000 of 34930\n",
      "Review 11000 of 34930\n",
      "Review 12000 of 34930\n",
      "Review 13000 of 34930\n",
      "Review 14000 of 34930\n",
      "Review 15000 of 34930\n",
      "Review 16000 of 34930\n",
      "Review 17000 of 34930\n",
      "Review 18000 of 34930\n",
      "Review 19000 of 34930\n",
      "Review 20000 of 34930\n",
      "Review 21000 of 34930\n",
      "Review 22000 of 34930\n",
      "Review 23000 of 34930\n",
      "Review 24000 of 34930\n",
      "Review 25000 of 34930\n",
      "Review 26000 of 34930\n",
      "Review 27000 of 34930\n",
      "Review 28000 of 34930\n",
      "Review 29000 of 34930\n",
      "Review 30000 of 34930\n",
      "Review 31000 of 34930\n",
      "Review 32000 of 34930\n",
      "Review 33000 of 34930\n",
      "Review 34000 of 34930\n",
      "(18070, 300)\n"
     ]
    }
   ],
   "source": [
    "# We also need to prepare the word2vec features, so that they are \n",
    "# each word is now mapped to an index, consistents with the training embedding \n",
    "# Currently, we are limiting each song to 500 words and padding on the left. \n",
    "features = get_indices_word2vec(songs_sample_train, \"full_lyrics\", model, maxLength=500,\n",
    "                         writeIndexFileName=\"songs2vec_indices.pickle\", padLeft=True )\n",
    "\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we separate data for training and validation \n",
    "y = songs_sample_train[\"chart\"]\n",
    "X_train, y_train, X_test, y_test = data_split.train_test_split_shuffle(y, features, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nites\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\nites\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=64, kernel_size=5, strides=1, padding=\"valid\")`\n",
      "C:\\Users\\nites\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=4)`\n",
      "C:\\Users\\nites\\Anaconda3\\lib\\site-packages\\keras\\models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31437 samples, validate on 3493 samples\n",
      "Epoch 1/3\n",
      "31437/31437 [==============================] - 604s - loss: 0.2967 - acc: 0.9006 - val_loss: 0.2625 - val_acc: 0.9007\n",
      "Epoch 2/3\n",
      "31437/31437 [==============================] - 605s - loss: 0.2179 - acc: 0.9162 - val_loss: 0.2403 - val_acc: 0.9095\n",
      "Epoch 3/3\n",
      "31437/31437 [==============================] - 607s - loss: 0.1278 - acc: 0.9568 - val_loss: 0.2133 - val_acc: 0.9330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2662f4d9b38>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN with LSTM\n",
    "n_symbols = len(embedding_weights)\n",
    "vocab_dim = len(embedding_weights[0])\n",
    "# vocab_dim is basically the dimension of each embedding vector; 300 by default.\n",
    "# input_dim is number of words in vocabulary +1\n",
    "\n",
    "# Create model using Keras code\n",
    "ep3_model = Sequential()  \n",
    "\n",
    "# embedding weights is the dictionary\n",
    "# change mask_zero to False\n",
    "ep3_model.add(Embedding(output_dim=vocab_dim, input_dim=n_symbols, mask_zero=False, weights=[embedding_weights], dropout=0.2))  \n",
    "\n",
    "# Add CNN layer\n",
    "nb_filter=64\n",
    "filter_length=5\n",
    "ep3_model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                            filter_length=filter_length,\n",
    "                            border_mode='valid',\n",
    "                            activation='relu',\n",
    "                            subsample_length=1))\n",
    "\n",
    "pool_length=4\n",
    "ep3_model.add(MaxPooling1D(pool_length=pool_length))\n",
    "\n",
    "# Add LSTM layer\n",
    "dense_dim=100\n",
    "ep3_model.add(LSTM(dense_dim))\n",
    "# return_sequences=False))\n",
    "ep3_model.add(Dense(1))\n",
    "ep3_model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "ep3_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "nb_epoch=3\n",
    "batch_size=64\n",
    "\n",
    "ep3_model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving Generated Model\n",
    "ep3_model.save('ep3_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating TensorBoard Output\n",
    "tbCallBack = TensorBoard(log_dir='Graph', histogram_freq=0,  \n",
    "          write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbCallBack.set_model(ep3_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Loading saved model to get predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ep3_model = load_model('ep3_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 11644\n",
      "Review 1000 of 11644\n",
      "Review 2000 of 11644\n",
      "Review 3000 of 11644\n",
      "Review 4000 of 11644\n",
      "Review 5000 of 11644\n",
      "Review 6000 of 11644\n",
      "Review 7000 of 11644\n",
      "Review 8000 of 11644\n",
      "Review 9000 of 11644\n",
      "Review 10000 of 11644\n",
      "Review 11000 of 11644\n"
     ]
    }
   ],
   "source": [
    "# Indexing words to features for the test data\n",
    "features_test = get_indices_word2vec(songs_sample_test, \"full_lyrics\", model, maxLength=500,\n",
    "                         writeIndexFileName=\"songs2vec_indices_test.pickle\", padLeft=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11644/11644 [==============================] - 77s    \n"
     ]
    }
   ],
   "source": [
    "# Creating predictions on test data\n",
    "test_predict = ep3_model.predict_classes(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = songs_sample_test[\"chart\"]\n",
    "y_test = y_test.as_matrix(columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.07%\n",
      "[[10263   176]\n",
      " [  631   574]]\n"
     ]
    }
   ],
   "source": [
    "# Performance on the test set\n",
    "\n",
    "accuracy = accuracy_score(y_test, test_predict)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "\n",
    "cnf_rf = confusion_matrix(y_test, test_predict)\n",
    "print(cnf_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6304/11644 [===============>..............] - ETA: 32s"
     ]
    }
   ],
   "source": [
    "# Get the predicted probabilities \n",
    "# and the FPR and TPR\n",
    "\n",
    "y_pred_cnn = ep3_model.predict_proba(features_test)\n",
    "CNN_auc = roc_auc_score(y_test, y_pred_cnn)\n",
    "print(\"AUC: %.2f%%\" % (CNN_auc * 100.0))\n",
    "CNN_fpr, CNN_tpr, CNN_thresholds = roc_curve(y_test, y_pred_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writing TPR and FPR probabilities for combined ROC\n",
    "np.savetxt(\"CNN_fpr.csv\", CNN_fpr, delimiter=\",\")\n",
    "np.savetxt(\"CNN_tpr.csv\", CNN_tpr, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Plot\n",
    "sns.set('talk', 'whitegrid', 'dark', font_scale=1.5, font='Arial',\n",
    "        rc={\"lines.linewidth\": 2, 'grid.linestyle': '--'})\n",
    "\n",
    "\n",
    "lw = 2\n",
    "plt.figure(figsize = (14,10))\n",
    "plt.plot(CNN_fpr, CNN_tpr, color='darkorange',\n",
    "         lw=lw, label='CNN with LSTM (AUC = %0.2f)' % CNN_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve: CNN with LSTM')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "#plt.savefig('roc_auc.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "400"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "612px",
    "left": "1283.67px",
    "right": "20px",
    "top": "115px",
    "width": "349px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
